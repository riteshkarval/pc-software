{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bb12cf1-0402-4876-9be4-a5e39d39a36d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json, os\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34122567-9199-44fb-a43f-4f71f34dbb54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_captions_map(datadir):\n",
    "    captions = list()\n",
    "    image_files = list()\n",
    "    imagedir = datadir\n",
    "    for file_type in (\".png\", \".jpg\"):\n",
    "        image_files.extend(glob.glob(os.path.join(imagedir, \"**/*\" + file_type), recursive=True))\n",
    "    for each_image_file in image_files:\n",
    "        caption_map = {\"file_name\":os.path.basename(each_image_file)}\n",
    "        filebasename, _ = os.path.splitext(each_image_file)\n",
    "        label_file_name = filebasename.replace(\"IMAGES\", \"TEXT_LABELS\") + \".gui\"\n",
    "        with open(label_file_name) as f:\n",
    "            caption_map[\"text\"] = f.read()\n",
    "        captions.append(caption_map)\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ede2c153-2b5e-4d2c-aeaf-b63073b0e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"../D2/IMAGES/\"\n",
    "captions = create_captions_map(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d7e915b-20a6-49e0-99ec-de0e4646910d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(root + \"metadata.jsonl\", 'w') as f:\n",
    "    for item in captions:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01e67122-6003-4cb1-9f7d-c8db22cf5dc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7fd80b349be4b63a9fe2926ee169a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imagefolder/default to /Users/ritesh/.cache/huggingface/datasets/imagefolder/default-9765dbf543442d46/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c63631e76e6403aadcf4b26aab5c3c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec87f255fb649d9a3c065eb0e131310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aadbe97d727f4a09b3a2e6726c19e827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7108d765d0d54623b057cdd76fd1f7cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imagefolder downloaded and prepared to /Users/ritesh/.cache/huggingface/datasets/imagefolder/default-9765dbf543442d46/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=root, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c74181c4-8e25-4d55-96fe-d2e7b390d37f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'text'],\n",
       "    num_rows: 300\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a045c7bf-159a-4d50-96ac-d37cf1df975f",
   "metadata": {},
   "source": [
    "# Create PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ff31631-a5d5-4960-b4df-c976d6f269ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ImageCaptioningDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "\n",
    "        encoding = self.processor(images=item[\"image\"], text=item[\"text\"], padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "        # remove batch dimension\n",
    "        encoding = {k:v.squeeze() for k,v in encoding.items()}\n",
    "\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a6a465d-7e7f-4578-9ea5-9a6482cc622f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-16 17:26:24.271220: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d18f26a-bf3b-4a56-98a8-cbacfddb695b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = ImageCaptioningDataset(dataset, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "971e2c3b-a2ca-491a-9e39-ea4a904e8764",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([512])\n",
      "attention_mask torch.Size([512])\n",
      "pixel_values torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "item = train_dataset[0]\n",
    "for k,v in item.items():\n",
    "  print(k,v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde751e8-6a20-4ca7-98f9-e6fcadfaf0ad",
   "metadata": {},
   "source": [
    "# Create PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76721019-c1c8-4434-939b-a897beb6a8d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6170ffb8-3da2-4585-b3ec-758eba4da382",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([2, 512])\n",
      "attention_mask torch.Size([2, 512])\n",
      "pixel_values torch.Size([2, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "for k,v in batch.items():\n",
    "  print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efc35528-7efa-4f1c-9a13-7287d3ee208a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "MEAN = np.array([123.675, 116.280, 103.530]) / 255\n",
    "STD = np.array([58.395, 57.120, 57.375]) / 255\n",
    "\n",
    "unnormalized_image = (batch[\"pixel_values\"][0].numpy() * np.array(STD)[:, None, None]) + np.array(MEAN)[:, None, None]\n",
    "unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
    "unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a34ac46-fb24-41a7-b885-bb0ebf106579",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1883c9e-e87a-4fdc-93be-f37eab23f949",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad4caad-a0c9-48a7-bff6-02dd60ac8265",
   "metadata": {},
   "source": [
    "# Dummy forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32e3bf03-3476-4c34-ad1f-4e4424df1194",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.8573, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                pixel_values=batch[\"pixel_values\"],\n",
    "                labels=batch[\"input_ids\"])\n",
    "outputs.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d783c28f-4333-42ca-bb9b-046a0fb737c9",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80a9f731-5ed6-43b8-994e-e536345dea58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: 10.601245880126953\tLoss: 10.077961921691895\tLoss: 8.821277618408203\tLoss: 9.026521682739258\tLoss: 8.4389009475708\tLoss: 7.936983585357666\tLoss: 8.077631950378418\tLoss: 7.7701897621154785\tLoss: 7.318760871887207\tLoss: 7.7890472412109375\tLoss: 7.737488746643066\tLoss: 7.741630554199219\tLoss: 7.052277565002441\tLoss: 7.106978893280029\tLoss: 7.274380207061768\tLoss: 7.345982074737549\tLoss: 7.2522687911987305\tLoss: 6.4005303382873535\tLoss: 6.762138366699219\tLoss: 6.156841278076172\tLoss: 5.777076721191406\tLoss: 5.913422107696533\tLoss: 5.917724132537842\tLoss: 5.883978366851807\tLoss: 5.446536540985107\tLoss: 6.233072280883789\tLoss: 5.144800186157227\tLoss: 5.260801792144775\tLoss: 5.381277084350586\tLoss: 5.494431972503662\tLoss: 5.93348503112793\tLoss: 4.84732723236084\tLoss: 5.259089469909668\tLoss: 4.942032337188721\tLoss: 4.595408916473389\tLoss: 4.702183723449707\tLoss: 4.99065637588501\tLoss: 4.407902240753174\tLoss: 4.091010570526123\tLoss: 4.930829048156738\tLoss: 4.083763599395752\tLoss: 4.695131301879883\tLoss: 4.477165699005127\tLoss: 4.060750961303711\tLoss: 3.7448248863220215\tLoss: 3.9501731395721436\tLoss: 3.683337688446045\tLoss: 3.8683199882507324\tLoss: 3.922802209854126\tLoss: 3.6444544792175293\tLoss: 3.409616231918335\tLoss: 3.2494823932647705\tLoss: 3.4969775676727295\tLoss: 3.187838315963745\tLoss: 3.237950563430786\tLoss: 2.9037411212921143\tLoss: 2.807581901550293\tLoss: 2.8385276794433594\tLoss: 2.7684803009033203\tLoss: 2.4658915996551514\tLoss: 2.3848743438720703\tLoss: 2.284287452697754\tLoss: 2.2616233825683594\tLoss: 2.2806453704833984\tLoss: 2.277616500854492\tLoss: 2.1045584678649902\tLoss: 1.9814540147781372\tLoss: 1.719621181488037\tLoss: 1.8640698194503784\tLoss: 1.8271290063858032\tLoss: 1.5844838619232178\tLoss: 1.5482898950576782\tLoss: 1.5438987016677856\tLoss: 1.490675449371338\tLoss: 1.4368749856948853\tLoss: 1.2139939069747925\tLoss: 1.1985934972763062\tLoss: 1.073681116104126\tLoss: 1.0206342935562134\tLoss: 0.9809765219688416\tLoss: 0.9665141701698303\tLoss: 0.9334608912467957\tLoss: 0.8742901682853699\tLoss: 0.7755775451660156\tLoss: 0.7673581838607788\tLoss: 0.6825574636459351\tLoss: 0.661323606967926\tLoss: 0.6247439980506897\tLoss: 0.5743882656097412\tLoss: 0.581058919429779\tLoss: 0.5155458450317383\tLoss: 0.5166711211204529\tLoss: 0.5194313526153564\tLoss: 0.4453781247138977\tLoss: 0.4474799931049347\tLoss: 0.41021835803985596\tLoss: 0.40432584285736084\tLoss: 0.38025206327438354\tLoss: 0.35018640756607056\tLoss: 0.3422752320766449\tLoss: 0.3275046944618225\tLoss: 0.3268643021583557\tLoss: 0.3040967583656311\tLoss: 0.278290718793869\tLoss: 0.2928702235221863\tLoss: 0.27405238151550293\tLoss: 0.2964620590209961\tLoss: 0.25598084926605225\tLoss: 0.28408852219581604\tLoss: 0.23467795550823212\tLoss: 0.25120413303375244\tLoss: 0.2340073138475418\tLoss: 0.24875518679618835\tLoss: 0.22572925686836243\tLoss: 0.20166613161563873\tLoss: 0.1926896870136261\tLoss: 0.20680983364582062\tLoss: 0.20629136264324188\tLoss: 0.20757253468036652\tLoss: 0.2017340213060379\tLoss: 0.18656136095523834\tLoss: 0.20351573824882507\tLoss: 0.19914700090885162\tLoss: 0.179936021566391\tLoss: 0.17220041155815125\tLoss: 0.18203192949295044\tLoss: 0.1475788950920105\tLoss: 0.16185028851032257\tLoss: 0.16936428844928741\tLoss: 0.1564381718635559\tLoss: 0.18102812767028809\tLoss: 0.18205906450748444\tLoss: 0.1795148402452469\tLoss: 0.15803053975105286\tLoss: 0.14672128856182098\tLoss: 0.18824343383312225\tLoss: 0.16972099244594574\tLoss: 0.15110072493553162\tLoss: 0.17543981969356537\tLoss: 0.14802362024784088\tLoss: 0.16272014379501343\tLoss: 0.17878320813179016\tLoss: 0.12787532806396484\tLoss: 0.1309826672077179\tLoss: 0.13438303768634796\tLoss: 0.1771019697189331\tLoss: 0.15087835490703583\tLoss: 0.12072370946407318\tLoss: 0.13619455695152283\tLoss: 0.10923955589532852\t"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(1):\n",
    "  print(\"Epoch:\", epoch)\n",
    "  for idx, batch in enumerate(train_dataloader):\n",
    "    input_ids = batch.pop(\"input_ids\").to(device)\n",
    "    pixel_values = batch.pop(\"pixel_values\").to(device)\n",
    "\n",
    "    outputs = model(input_ids=input_ids,\n",
    "                    pixel_values=pixel_values,\n",
    "                    labels=input_ids)\n",
    "    \n",
    "    loss = outputs.loss\n",
    "\n",
    "    print(\"Loss:\", loss.item(), end=\"\\t\")\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75928f96-83fb-4a60-b286-c1154f5e37ca",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0b62a74-a5f1-4ab3-b22d-6260abbb5c76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "header { search - bar } sidebar { medium - title, radio, icons } canvas - header { btn - inactive, btn - inactive, btn - inactive, btn - inactive, btn - inactive } row { small\n"
     ]
    }
   ],
   "source": [
    "# prepare image for the model\n",
    "example = dataset[0]\n",
    "image = example[\"image\"]\n",
    "width, height = image.size\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "pixel_values = inputs.pixel_values\n",
    "\n",
    "generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n",
    "generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(generated_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772b52f9-c554-4dbb-b6c1-a87aff5e9bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "purecode",
   "language": "python",
   "name": "purecode"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
