{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bb12cf1-0402-4876-9be4-a5e39d39a36d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json, os\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34122567-9199-44fb-a43f-4f71f34dbb54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_captions_map(datadir):\n",
    "    captions = list()\n",
    "    image_files = list()\n",
    "    imagedir = datadir\n",
    "    for file_type in (\".png\", \".jpg\"):\n",
    "        image_files.extend(glob.glob(os.path.join(imagedir, \"**/*\" + file_type), recursive=True))\n",
    "    for each_image_file in image_files:\n",
    "        caption_map = {\"file_name\":os.path.basename(each_image_file)}\n",
    "        filebasename, _ = os.path.splitext(each_image_file)\n",
    "        label_file_name = filebasename.replace(\"IMAGES\", \"TEXT_LABELS\") + \".gui\"\n",
    "        with open(label_file_name) as f:\n",
    "            caption_map[\"text\"] = f.read()\n",
    "        captions.append(caption_map)\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ede2c153-2b5e-4d2c-aeaf-b63073b0e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"../D1/IMAGES/\"\n",
    "captions = create_captions_map(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d7e915b-20a6-49e0-99ec-de0e4646910d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(root + \"metadata.jsonl\", 'w') as f:\n",
    "    for item in captions:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01e67122-6003-4cb1-9f7d-c8db22cf5dc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff0260b022c48d8a5c5adb8138f1366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imagefolder/default to /Users/ritesh/.cache/huggingface/datasets/imagefolder/default-ce6e299874604f3b/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1416d1076b864b75ab1fef061da33725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de949c840b94923aa97ee9e47d1a828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f639cb8ca6d146c9ae16af962109bb6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12b9679a4934d508e128b194db8f800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imagefolder downloaded and prepared to /Users/ritesh/.cache/huggingface/datasets/imagefolder/default-ce6e299874604f3b/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=root, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c74181c4-8e25-4d55-96fe-d2e7b390d37f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'text'],\n",
       "    num_rows: 300\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a045c7bf-159a-4d50-96ac-d37cf1df975f",
   "metadata": {},
   "source": [
    "# Create PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ff31631-a5d5-4960-b4df-c976d6f269ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ImageCaptioningDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "\n",
    "        encoding = self.processor(images=item[\"image\"], text=item[\"text\"], padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "        # remove batch dimension\n",
    "        encoding = {k:v.squeeze() for k,v in encoding.items()}\n",
    "\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a6a465d-7e7f-4578-9ea5-9a6482cc622f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-16 16:56:21.492409: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe4af468edc540d9bd588290e67bb317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/453 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6071dcdcd0b44e18d5b50ff90b6bbf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29908718e47d47188bbb405e90adee4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d715d2f0162c4bdb8267108e43d7d93d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d18f26a-bf3b-4a56-98a8-cbacfddb695b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = ImageCaptioningDataset(dataset, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "971e2c3b-a2ca-491a-9e39-ea4a904e8764",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([512])\n",
      "attention_mask torch.Size([512])\n",
      "pixel_values torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "item = train_dataset[0]\n",
    "for k,v in item.items():\n",
    "  print(k,v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde751e8-6a20-4ca7-98f9-e6fcadfaf0ad",
   "metadata": {},
   "source": [
    "# Create PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76721019-c1c8-4434-939b-a897beb6a8d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6170ffb8-3da2-4585-b3ec-758eba4da382",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([2, 512])\n",
      "attention_mask torch.Size([2, 512])\n",
      "pixel_values torch.Size([2, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "for k,v in batch.items():\n",
    "  print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efc35528-7efa-4f1c-9a13-7287d3ee208a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "MEAN = np.array([123.675, 116.280, 103.530]) / 255\n",
    "STD = np.array([58.395, 57.120, 57.375]) / 255\n",
    "\n",
    "unnormalized_image = (batch[\"pixel_values\"][0].numpy() * np.array(STD)[:, None, None]) + np.array(MEAN)[:, None, None]\n",
    "unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
    "unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a34ac46-fb24-41a7-b885-bb0ebf106579",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1883c9e-e87a-4fdc-93be-f37eab23f949",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e458140c50546fb938ece4de2b9e37c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/2.82k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87547da229e4853bc437c51294b191f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/707M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45bfdde78a0456ea7631d8c1d6f4ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad4caad-a0c9-48a7-bff6-02dd60ac8265",
   "metadata": {},
   "source": [
    "# Dummy forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32e3bf03-3476-4c34-ad1f-4e4424df1194",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.0119, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                pixel_values=batch[\"pixel_values\"],\n",
    "                labels=batch[\"input_ids\"])\n",
    "outputs.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d783c28f-4333-42ca-bb9b-046a0fb737c9",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80a9f731-5ed6-43b8-994e-e536345dea58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: 5.870874881744385\tLoss: 6.350809574127197\tLoss: 5.763431549072266\tLoss: 5.4379801750183105\tLoss: 5.4998626708984375\tLoss: 5.20180606842041\tLoss: 5.197664737701416\tLoss: 5.492949962615967\tLoss: 5.431489944458008\tLoss: 4.7418107986450195\tLoss: 4.775285243988037\tLoss: 4.613325595855713\tLoss: 4.9958367347717285\tLoss: 4.673680782318115\tLoss: 4.680605888366699\tLoss: 4.9198174476623535\tLoss: 4.28180456161499\tLoss: 4.108408451080322\tLoss: 4.286749362945557\tLoss: 3.997328996658325\tLoss: 3.8612287044525146\tLoss: 3.7262680530548096\tLoss: 3.906266450881958\tLoss: 3.7782742977142334\tLoss: 3.741389274597168\tLoss: 3.4223849773406982\tLoss: 3.821803569793701\tLoss: 3.2485148906707764\tLoss: 3.22570538520813\tLoss: 3.149899959564209\tLoss: 2.985762596130371\tLoss: 3.10326886177063\tLoss: 3.145672082901001\tLoss: 2.7723686695098877\tLoss: 2.684929609298706\tLoss: 2.649779796600342\tLoss: 2.489927291870117\tLoss: 2.4839365482330322\tLoss: 2.327587604522705\tLoss: 2.2354230880737305\tLoss: 2.186319351196289\tLoss: 1.9590864181518555\tLoss: 2.0242292881011963\tLoss: 2.193843126296997\tLoss: 1.9674543142318726\tLoss: 1.9630123376846313\tLoss: 1.9880949258804321\tLoss: 1.647971510887146\tLoss: 1.572733759880066\tLoss: 1.7051398754119873\tLoss: 1.5170888900756836\tLoss: 1.4203943014144897\tLoss: 1.294296383857727\tLoss: 1.232966661453247\tLoss: 1.1590319871902466\tLoss: 1.0980736017227173\tLoss: 1.0835645198822021\tLoss: 0.9586696028709412\tLoss: 0.8879559636116028\tLoss: 0.9373969435691833\tLoss: 0.8644698262214661\tLoss: 0.7787712216377258\tLoss: 0.7059952020645142\tLoss: 0.7331081628799438\tLoss: 0.6793580651283264\tLoss: 0.6168447136878967\tLoss: 0.5675908923149109\tLoss: 0.5747402906417847\tLoss: 0.5118199586868286\tLoss: 0.5035996437072754\tLoss: 0.47259625792503357\tLoss: 0.4505464434623718\tLoss: 0.43421250581741333\tLoss: 0.3947746157646179\tLoss: 0.38768669962882996\tLoss: 0.38350728154182434\tLoss: 0.33153969049453735\tLoss: 0.3394571840763092\tLoss: 0.3324434161186218\tLoss: 0.3256681263446808\tLoss: 0.28797826170921326\tLoss: 0.2899631857872009\tLoss: 0.26915788650512695\tLoss: 0.2528696358203888\tLoss: 0.2530747950077057\tLoss: 0.25453901290893555\tLoss: 0.23366796970367432\tLoss: 0.233461394906044\tLoss: 0.2250424176454544\tLoss: 0.21121561527252197\tLoss: 0.20900341868400574\tLoss: 0.2079659104347229\tLoss: 0.18159274756908417\tLoss: 0.18045344948768616\tLoss: 0.1832982450723648\tLoss: 0.1713649183511734\tLoss: 0.16196785867214203\tLoss: 0.1618499457836151\tLoss: 0.1754612773656845\tLoss: 0.17458689212799072\tLoss: 0.14488725364208221\tLoss: 0.14198416471481323\tLoss: 0.14617563784122467\tLoss: 0.1353064775466919\tLoss: 0.15175841748714447\tLoss: 0.15235482156276703\tLoss: 0.14473994076251984\tLoss: 0.12650415301322937\tLoss: 0.13611845672130585\tLoss: 0.13671112060546875\tLoss: 0.1231289729475975\tLoss: 0.14823900163173676\tLoss: 0.11310376971960068\tLoss: 0.12762877345085144\tLoss: 0.12245331704616547\tLoss: 0.11150846630334854\tLoss: 0.12078869342803955\tLoss: 0.12252373993396759\tLoss: 0.1121678575873375\tLoss: 0.11097818613052368\tLoss: 0.12018675357103348\tLoss: 0.09619221091270447\tLoss: 0.10029496997594833\tLoss: 0.1111106425523758\tLoss: 0.11219602078199387\tLoss: 0.09792620688676834\tLoss: 0.09456796944141388\tLoss: 0.10186029225587845\tLoss: 0.09361595660448074\tLoss: 0.1037498265504837\tLoss: 0.09211599826812744\tLoss: 0.09480652213096619\tLoss: 0.1149161159992218\tLoss: 0.08884363621473312\tLoss: 0.10271895676851273\tLoss: 0.1229265034198761\tLoss: 0.09565981477499008\tLoss: 0.10564015805721283\tLoss: 0.08803153783082962\tLoss: 0.09302464872598648\tLoss: 0.09603647142648697\tLoss: 0.08079737424850464\tLoss: 0.08467528969049454\tLoss: 0.07653360813856125\tLoss: 0.10847162455320358\tLoss: 0.09454522281885147\tLoss: 0.09197980165481567\tLoss: 0.12222672998905182\tLoss: 0.08476179093122482\tLoss: 0.09163332730531693\t"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(1):\n",
    "  print(\"Epoch:\", epoch)\n",
    "  for idx, batch in enumerate(train_dataloader):\n",
    "    input_ids = batch.pop(\"input_ids\").to(device)\n",
    "    pixel_values = batch.pop(\"pixel_values\").to(device)\n",
    "\n",
    "    outputs = model(input_ids=input_ids,\n",
    "                    pixel_values=pixel_values,\n",
    "                    labels=input_ids)\n",
    "    \n",
    "    loss = outputs.loss\n",
    "\n",
    "    print(\"Loss:\", loss.item(), end=\"\\t\")\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75928f96-83fb-4a60-b286-c1154f5e37ca",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0b62a74-a5f1-4ab3-b22d-6260abbb5c76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "header { btn - active, btn - inactive, btn - inactive, btn - inactive, btn - inactive } row { quadruple { small - title, text, btn - orange } quadruple { small\n"
     ]
    }
   ],
   "source": [
    "# prepare image for the model\n",
    "example = dataset[0]\n",
    "image = example[\"image\"]\n",
    "width, height = image.size\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "pixel_values = inputs.pixel_values\n",
    "\n",
    "generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n",
    "generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(generated_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772b52f9-c554-4dbb-b6c1-a87aff5e9bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "purecode",
   "language": "python",
   "name": "purecode"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
